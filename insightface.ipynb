{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910a3832",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d1405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51a2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = \"temp_captures\"\n",
    "target_size = (224,244)\n",
    "threshold = 0.5\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ab84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize temporary image folder\n",
    "if not os.path.exists(temp_folder):\n",
    "    os.makedirs(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ffc280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Initialize face detector\n",
    "detector = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider'])\n",
    "detector.prepare(ctx_id=0, det_size=(640, 640), det_thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8b1ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionSmoother:\n",
    "    def __init__(self, window_size=window_size):\n",
    "        self.window_size = window_size\n",
    "        self.history = []\n",
    "    \n",
    "    def add_recognition(self, person_id, score):\n",
    "        self.history.append((person_id, score))\n",
    "        if len(self.history) > self.window_size:\n",
    "            self.history.pop(0)\n",
    "    \n",
    "    def get_smoothed_result(self):\n",
    "        if not self.history:\n",
    "            return None, 0\n",
    "\n",
    "        weights = np.linspace(0.5, 1.5, len(self.history))\n",
    "        scores = {}\n",
    "        \n",
    "        for (pid, score), weight in zip(self.history, weights):\n",
    "            if pid not in scores:\n",
    "                scores[pid] = []\n",
    "            scores[pid].append(score * weight)\n",
    "        \n",
    "        avg_scores = {pid: np.mean(vals) for pid, vals in scores.items()}\n",
    "        best_pid = max(avg_scores.items(), key=lambda x: x[1])[0]\n",
    "        best_score = avg_scores[best_pid]\n",
    "        \n",
    "        return best_pid, best_score\n",
    "\n",
    "# Initialize smoother\n",
    "smoother = RecognitionSmoother(window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75122624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"jesmine0820/assignment_face_recognition\",   \n",
    "    filename=\"face_embeddings.pkl\",  \n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Load the pickle file\n",
    "with open(file_path, \"rb\") as f:\n",
    "    embeddings_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967214b9",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdbc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the brightness\n",
    "def detect_brightness(image):\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    return np.mean(gray)\n",
    "\n",
    "# Adjust gamma\n",
    "def adjust_gamma(image, gamma):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in np.arange(256)]).astype(\"uint8\")\n",
    "    return cv.LUT(image, table)\n",
    "\n",
    "def preprocess_image(img, target_size=target_size):\n",
    "\n",
    "    # Gamma correction based on brightness\n",
    "    brightness = detect_brightness(img)\n",
    "    if brightness > 180:\n",
    "        img = adjust_gamma(img, gamma=1.5)\n",
    "    elif brightness < 70:\n",
    "        img = adjust_gamma(img, gamma=0.5)\n",
    "        \n",
    "    # Normalize and resize\n",
    "    img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img_resized = cv.resize(img_rgb, target_size, interpolation=cv.INTER_AREA)\n",
    "    \n",
    "    # Smart blurring\n",
    "    if cv.Laplacian(img_resized, cv.CV_64F).var() < 100:\n",
    "        img_resized = cv.GaussianBlur(img_resized, (3, 3), 0)\n",
    "    \n",
    "    return img_resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9823f5",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68930bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_best_face(image):\n",
    "    faces = detector.get(image)\n",
    "    if not faces:\n",
    "        return None, None\n",
    "\n",
    "    img_center = np.array([image.shape[1] / 2, image.shape[0] / 2])\n",
    "    \n",
    "    # Score faces based on multiple factors\n",
    "    scored_faces = []\n",
    "    for face in faces:\n",
    "        bbox = face.bbox.astype(int)\n",
    "        \n",
    "        # Center proximity \n",
    "        face_center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n",
    "        center_score = 1 - (np.linalg.norm(face_center - img_center) / \n",
    "                         np.linalg.norm(img_center))\n",
    "        \n",
    "        # Face size \n",
    "        face_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "        size_score = face_area / (image.shape[0] * image.shape[1])\n",
    "        \n",
    "        # Detection confidence \n",
    "        det_score = face.det_score\n",
    "        \n",
    "        # Sharpness\n",
    "        face_roi = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        sharpness = cv.Laplacian(cv.cvtColor(face_roi, cv.COLOR_BGR2GRAY), cv.CV_64F).var()\n",
    "        sharpness_score = sharpness / 1000\n",
    "        \n",
    "        total_score = (0.4 * center_score + 0.3 * size_score + \n",
    "                      0.2 * det_score + 0.1 * sharpness_score)\n",
    "        \n",
    "        scored_faces.append((total_score, face))\n",
    "    \n",
    "    if not scored_faces:\n",
    "        return None, None\n",
    "        \n",
    "    scored_faces.sort(reverse=True, key=lambda x: x[0])\n",
    "    best_face = scored_faces[0][1]\n",
    "    bbox = best_face.bbox.astype(int)\n",
    "    cropped_face = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    \n",
    "    return cropped_face, best_face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874c2bf",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dcd922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embedding_from_obj(face_obj):\n",
    "    return face_obj.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f309db",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8660dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(embedding, dataset):\n",
    "    best_score = -1\n",
    "    best_id = None\n",
    "    best_name = None\n",
    "    \n",
    "    for entry in dataset:\n",
    "        db_embedding = entry[\"embedding\"]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        cos_sim = np.dot(embedding, db_embedding) / (\n",
    "            np.linalg.norm(embedding) * np.linalg.norm(db_embedding)\n",
    "        )\n",
    "        \n",
    "        # Euclidean distance \n",
    "        eucl_dist = np.linalg.norm(embedding - db_embedding)\n",
    "        eucl_sim = 1 / (1 + eucl_dist) \n",
    "        \n",
    "        # Combined score \n",
    "        similarity = 0.7 * cos_sim + 0.3 * eucl_sim\n",
    "        \n",
    "        if similarity > best_score:\n",
    "            best_score = similarity\n",
    "            best_id = entry[\"id\"]\n",
    "            best_name = entry[\"image_name\"]\n",
    "    \n",
    "    return best_id, best_name, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e3c45",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176f64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_result(image, name, score):\n",
    "    faces = detector.get(image)\n",
    "\n",
    "    if not faces:\n",
    "        return image\n",
    "\n",
    "    # Image center\n",
    "    h, w, _ = image.shape\n",
    "    img_center = np.array([w // 2, h // 2])\n",
    "\n",
    "    # Find face closest to center\n",
    "    closest_face = None\n",
    "    min_dist = float('inf')\n",
    "\n",
    "    for face in faces:\n",
    "        bbox = face.bbox.astype(int)\n",
    "        face_center = np.array([(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2])\n",
    "        dist = np.linalg.norm(face_center - img_center)\n",
    "\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_face = face\n",
    "\n",
    "    if closest_face is None:\n",
    "        return image\n",
    "\n",
    "    # Get bounding box for closest face\n",
    "    bbox = closest_face.bbox.astype(int)\n",
    "\n",
    "    # Draw rectangle\n",
    "    cv.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "\n",
    "    # Create label\n",
    "    label = f\"{name} ({score:.2f})\"\n",
    "    cv.putText(image, label, (bbox[0], bbox[1] - 10),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c27be",
   "metadata": {},
   "source": [
    "# Image Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55abcbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect id: 24WMR08866\n",
      "Score: 0.38093771541249594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv.imread(\"test_photos/Jesmine/jesmine_1.jpg\")\n",
    "processed_img = preprocess_image(img)\n",
    "cropped_face, face_obj = crop_best_face(processed_img)\n",
    "embedding = get_face_embedding_from_obj(face_obj)\n",
    "person_id, name, score = recognize_face(embedding, embeddings_data)\n",
    "smoother.add_recognition(person_id, score)\n",
    "smoothed_id, smoothed_score = smoother.get_smoothed_result()\n",
    "frame = draw_result(img, name, smoothed_score)\n",
    "\n",
    "print(f\"Detect id: {person_id}\")\n",
    "print(f\"Score: {score}\")\n",
    "\n",
    "cv.imshow(\"Frame\", frame)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240b1e0",
   "metadata": {},
   "source": [
    "# Real Time Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ce789cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect id: 24WMR08866\n",
      "Score: 0.36\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.37\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.35\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.45\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.45\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.45\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.45\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.32\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.35\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.32\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.31\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.37\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.38\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.38\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.21\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.18\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.21\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.27\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.25\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.27\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.26\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.18\n",
      "Detect id: 24WMR08863\n",
      "Score: 0.26\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.33\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.37\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.40\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.44\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.35\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.33\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.35\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.35\n",
      "Detect id: 24WMR08837\n",
      "Score: 0.34\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.46\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.43\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.41\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.46\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.39\n",
      "Detect id: 24WMR08866\n",
      "Score: 0.42\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Crop and preprocess\u001b[39;00m\n\u001b[0;32m     26\u001b[0m processed_img \u001b[38;5;241m=\u001b[39m preprocess_image(frame)\n\u001b[1;32m---> 27\u001b[0m cropped_face, face_obj \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_best_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Get embedding\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m get_face_embedding_from_obj(face_obj)\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36mcrop_best_face\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Sharpness\u001b[39;00m\n\u001b[0;32m     26\u001b[0m face_roi \u001b[38;5;241m=\u001b[39m image[bbox[\u001b[38;5;241m1\u001b[39m]:bbox[\u001b[38;5;241m3\u001b[39m], bbox[\u001b[38;5;241m0\u001b[39m]:bbox[\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m---> 27\u001b[0m sharpness \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mLaplacian(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_roi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m, cv\u001b[38;5;241m.\u001b[39mCV_64F)\u001b[38;5;241m.\u001b[39mvar()\n\u001b[0;32m     28\u001b[0m sharpness_score \u001b[38;5;241m=\u001b[39m sharpness \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     30\u001b[0m total_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m center_score \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m*\u001b[39m size_score \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m     31\u001b[0m               \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m det_score \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m sharpness_score)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Open webcam\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv.flip(frame, 1)\n",
    "\n",
    "    # Detect faces with buffalo_l\n",
    "    faces = detector.get(frame)\n",
    "\n",
    "    if faces:\n",
    "        # Find face closest to center\n",
    "        h, w, _ = frame.shape\n",
    "        img_center = np.array([w // 2, h // 2])\n",
    "        closest_face = min(\n",
    "            faces,\n",
    "            key=lambda f: np.linalg.norm(\n",
    "                np.array([(f.bbox[0] + f.bbox[2]) / 2, (f.bbox[1] + f.bbox[3]) / 2]) - img_center\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Crop and preprocess\n",
    "        processed_img = preprocess_image(frame)\n",
    "        cropped_face, face_obj = crop_best_face(processed_img)\n",
    "\n",
    "        if face_obj is not None:\n",
    "            # Get embedding\n",
    "            embedding = get_face_embedding_from_obj(face_obj)\n",
    "\n",
    "            # Recognize face\n",
    "            person_id, name, score = recognize_face(embedding, embeddings_data)\n",
    "\n",
    "            # Smooth results\n",
    "            smoother.add_recognition(person_id, score)\n",
    "            smoothed_id, smoothed_score = smoother.get_smoothed_result()\n",
    "\n",
    "            # Draw results\n",
    "            frame = draw_result(frame, name, smoothed_score)\n",
    "\n",
    "            # Print info\n",
    "            print(f\"Detect id: {person_id}\")\n",
    "            print(f\"Score: {score:.2f}\")\n",
    "\n",
    "    # Draw middle rectangle\n",
    "    rect_w, rect_h = 200, 200\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    top_left = (center_x - rect_w // 2, center_y - rect_h // 2)\n",
    "    bottom_right = (center_x + rect_w // 2, center_y + rect_h // 2)\n",
    "    cv.rectangle(frame, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "    # Show real-time capture\n",
    "    cv.namedWindow(\"Real-Time Capture\", cv.WINDOW_NORMAL)\n",
    "    cv.imshow(\"Real-Time Capture\", frame)\n",
    "\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('s'):\n",
    "        # Save screenshot\n",
    "        filename = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".jpg\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        cv.imwrite(filepath, frame)\n",
    "        print(f\"Screenshot saved: {filepath}\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187af71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
