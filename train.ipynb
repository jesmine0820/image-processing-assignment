{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a33dc4",
   "metadata": {},
   "source": [
    "# Face Recognition - Jesmine Tey Khai Jing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f47e98",
   "metadata": {},
   "source": [
    "Buffalo-m: https://github.com/deepinsight/insightface/tree/master/python-package#model-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9bab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\User/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "Detected id: 24WMR08866, Score: 0.6124472290277481\n",
      "Detected id: 24WMR08866, Score: 0.6041479200124741\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"jesmine0820/assignment_face_recognition\",   \n",
    "    filename=\"face_embeddings.pkl\",  \n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    embeddings_data = pickle.load(f)\n",
    "\n",
    "detector = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider'])\n",
    "detector.prepare(ctx_id=0, det_size=(640, 640), det_thresh=0.5)\n",
    "\n",
    "class RecognitionSmoother:\n",
    "    def __init__(self, window_size=5):\n",
    "        self.window_size = window_size\n",
    "        self.history = []\n",
    "    \n",
    "    def add_recognition(self, person_id, score):\n",
    "        self.history.append((person_id, score))\n",
    "        if len(self.history) > self.window_size:\n",
    "            self.history.pop(0)\n",
    "    \n",
    "    def get_smoothed_result(self):\n",
    "        if not self.history:\n",
    "            return None, 0\n",
    "\n",
    "        weights = np.linspace(0.5, 1.5, len(self.history))\n",
    "        scores = {}\n",
    "        \n",
    "        for (pid, score), weight in zip(self.history, weights):\n",
    "            if pid not in scores:\n",
    "                scores[pid] = []\n",
    "            scores[pid].append(score * weight)\n",
    "        \n",
    "        avg_scores = {pid: np.mean(vals) for pid, vals in scores.items()}\n",
    "        best_pid = max(avg_scores.items(), key=lambda x: x[1])[0]\n",
    "        best_score = avg_scores[best_pid]\n",
    "        \n",
    "        return best_pid, best_score\n",
    "\n",
    "smoother = RecognitionSmoother(window_size=5)\n",
    "\n",
    "def enhance_contrast(img):\n",
    "    lab = cv.cvtColor(img, cv.COLOR_RGB2LAB)\n",
    "    l, a, b = cv.split(lab)\n",
    "    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged = cv.merge((cl, a, b))\n",
    "    return cv.cvtColor(merged, cv.COLOR_LAB2RGB)\n",
    "\n",
    "def align_face(image, face_obj, target_size=(112, 112)):\n",
    "    src = np.array([\n",
    "        [38.2946, 51.6963],\n",
    "        [73.5318, 51.5014],\n",
    "        [56.0252, 71.7366],\n",
    "        [41.5493, 92.3655],\n",
    "        [70.7299, 92.2041]], dtype=np.float32)\n",
    "\n",
    "    dst = face_obj.kps.astype(np.float32)\n",
    "    M = cv.estimateAffinePartial2D(dst, src, method=cv.LMEDS)[0]\n",
    "    aligned = cv.warpAffine(image, M, target_size, borderValue=0.0)\n",
    "    return aligned\n",
    "\n",
    "def get_face_embedding_from_obj(face_obj):\n",
    "    emb = face_obj.embedding\n",
    "    if emb is None:\n",
    "        return None\n",
    "    return emb / np.linalg.norm(emb)\n",
    "\n",
    "def recognize_face(embedding, dataset, threshold=0.5):\n",
    "    if embedding is None:\n",
    "        return None, None, -1\n",
    "\n",
    "    best_score = -1\n",
    "    best_id = None\n",
    "    best_name = None\n",
    "\n",
    "    for entry in dataset:\n",
    "        db_embedding = entry[\"embedding\"]\n",
    "        db_embedding = db_embedding / np.linalg.norm(db_embedding)\n",
    "\n",
    "        cos_sim = np.dot(embedding, db_embedding)\n",
    "        if cos_sim > best_score:\n",
    "            best_score = cos_sim\n",
    "            best_id = entry[\"id\"]\n",
    "            best_name = entry[\"image_name\"]\n",
    "\n",
    "    if best_score < threshold:\n",
    "        return None, None, best_score\n",
    "\n",
    "    return best_id, best_name, best_score\n",
    "\n",
    "def draw_result(image, name, score):\n",
    "    faces = detector.get(image)\n",
    "    if not faces:\n",
    "        return image\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    img_center = np.array([w // 2, h // 2])\n",
    "    closest_face, min_dist = None, float(\"inf\")\n",
    "\n",
    "    for face in faces:\n",
    "        bbox = face.bbox.astype(int)\n",
    "        face_center = np.array([(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2])\n",
    "        dist = np.linalg.norm(face_center - img_center)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_face = face\n",
    "\n",
    "    if closest_face is None:\n",
    "        return image\n",
    "\n",
    "    bbox = closest_face.bbox.astype(int)\n",
    "    cv.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "\n",
    "    label = f\"{name} ({score:.2f})\" if name else \"Unknown\"\n",
    "    cv.putText(image, label, (bbox[0], bbox[1] - 10),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "def real_time_pipeline():\n",
    "    current_person = None\n",
    "    start_time = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv.flip(frame, 1)\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        faces = detector.get(rgb_frame)\n",
    "\n",
    "        if faces:\n",
    "            # pick best face (highest det_score)\n",
    "            faces.sort(key=lambda f: f.det_score, reverse=True)\n",
    "            best_face = faces[0]\n",
    "\n",
    "            # face quality filtering\n",
    "            if best_face.det_score < 0.6:\n",
    "                continue\n",
    "            if (best_face.bbox[2] - best_face.bbox[0]) < 80:\n",
    "                continue\n",
    "\n",
    "            # align + preprocess\n",
    "            aligned_face = align_face(rgb_frame, best_face)\n",
    "            enhanced_face = enhance_contrast(aligned_face)\n",
    "\n",
    "            # get embedding\n",
    "            embedding = get_face_embedding_from_obj(best_face)\n",
    "\n",
    "            # recognize\n",
    "            person_id, name, score = recognize_face(embedding, embeddings_data)\n",
    "\n",
    "            # smooth results\n",
    "            smoother.add_recognition(person_id, score)\n",
    "            smoothed_id, smoothed_score = smoother.get_smoothed_result()\n",
    "\n",
    "            # draw\n",
    "            frame = draw_result(frame, name, smoothed_score)\n",
    "\n",
    "            # stable detection for 5s\n",
    "            if smoothed_id == current_person:\n",
    "                if start_time and (time.time() - start_time >= 5):\n",
    "                    print(f\"Detected id: {smoothed_id}, Score: {smoothed_score}\")\n",
    "                    start_time = None\n",
    "            else:\n",
    "                current_person = smoothed_id\n",
    "                start_time = time.time()\n",
    "\n",
    "        # draw middle guide box\n",
    "        h, w, _ = frame.shape\n",
    "        rect_w, rect_h = 200, 200\n",
    "        center_x, center_y = w // 2, h // 2\n",
    "        top_left = (center_x - rect_w // 2, center_y - rect_h // 2)\n",
    "        bottom_right = (center_x + rect_w // 2, center_y + rect_h // 2)\n",
    "        cv.rectangle(frame, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "        cv.imshow(\"Face Recognition\", frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "real_time_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7130c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_accuracy():\n",
    "    for folder in os.listdir(\"test_photos\"):\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(('.jpg')):\n",
    "                img_path = os.path.join(\"test_photos\",folder, filename)\n",
    "                img = cv.imread(img_path)\n",
    "                cv.imshow(\"image\", img)\n",
    "                cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f39b4a",
   "metadata": {},
   "source": [
    "# Face Recognition - Ethel Ng Yi Yan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5712a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize detector and embedder\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()\n",
    "\n",
    "def l2_normalize(x):\n",
    "    return x / np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be46bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing known faces\n",
    "photo_dir = r\"C:\\Users\\User\\Image Processing Assignment\\Photo\"\n",
    "\n",
    "# Get embedding from image path\n",
    "def get_face_embedding(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[WARNING] Failed to load image: {img_path}\")\n",
    "        return None\n",
    "    \n",
    "    results = detector.detect_faces(img)\n",
    "    if len(results) == 0:\n",
    "        print(f\"[INFO] No face detected in: {img_path}\")\n",
    "        return None\n",
    "    \n",
    "    face = results[0]\n",
    "    x, y, w, h = face['box']\n",
    "    x, y = max(0, x), max(0, y)\n",
    "    \n",
    "    face_img = img[y:y+h, x:x+w]\n",
    "    face_img = cv2.resize(face_img, (160, 160))\n",
    "    \n",
    "    embedding = embedder.embeddings([face_img])[0]\n",
    "    embedding = l2_normalize(embedding)\n",
    "    return embedding\n",
    "\n",
    "# Build the face database from photo directory\n",
    "def build_face_database(folder):\n",
    "    database = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            path = os.path.join(folder, file)\n",
    "            name = os.path.splitext(file)[0]\n",
    "            print(f\"Processing: {file}\")\n",
    "            embedding = get_face_embedding(path)\n",
    "            if embedding is not None:\n",
    "                database[name] = embedding\n",
    "            else:\n",
    "                print(f\"[SKIPPED] {file}\")\n",
    "    return database\n",
    "\n",
    "face_database = build_face_database(photo_dir)\n",
    "print(f\"âœ… Loaded {len(face_database)} valid faces from Photo folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed438ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top N matches with cosine similarity\n",
    "def get_top_matches(face_img, database, top_n=3):\n",
    "    face_img = cv2.resize(face_img, (160, 160))\n",
    "    embedding = embedder.embeddings([face_img])[0]\n",
    "    embedding = l2_normalize(embedding)\n",
    "\n",
    "    similarities = []\n",
    "    for name, db_emb in database.items():\n",
    "        sim_score = cosine_similarity([embedding], [db_emb])[0][0]  # Higher is better\n",
    "        similarities.append((name, sim_score))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Start webcam recognition\n",
    "video = cv2.VideoCapture(0)\n",
    "print(\"ğŸ“· Press 'q' to quit...\")\n",
    "\n",
    "threshold = 0.7  # Raised threshold\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = detector.detect_faces(frame)\n",
    "    \n",
    "    for face in results:\n",
    "        x, y, w, h = face['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "\n",
    "        # Add margin to bounding box\n",
    "        margin = 10\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(frame.shape[1], x + w + margin)\n",
    "        y2 = min(frame.shape[0], y + h + margin)\n",
    "\n",
    "        face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "        top_matches = get_top_matches(face_img, face_database)\n",
    "\n",
    "        if top_matches and top_matches[0][1] > threshold:\n",
    "            name = top_matches[0][0]\n",
    "            similarity = top_matches[0][1]\n",
    "            cv2.putText(frame, f\"{name} ({similarity*100:.1f}%)\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            print(f\"\\nTop 3 matches for face at ({x}, {y}):\")\n",
    "            for match_name, sim in top_matches:\n",
    "                print(f\"  {match_name}: {sim * 100:.2f}% similarity\")\n",
    "        else:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            print(f\"\\nâŒ No good match found for face at ({x}, {y}).\")\n",
    "\n",
    "    cv2.imshow('Face Recognition with MTCNN + FaceNet', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449f9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Processing: 24WMA08802.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Processing: 24WMA08803.jpg\n",
      "[INFO] No face detected in: photos\\24WMA08803.jpg\n",
      "[SKIPPED] 24WMA08803.jpg\n",
      "Processing: 24WMH08807.jpg\n",
      "[INFO] No face detected in: photos\\24WMH08807.jpg\n",
      "[SKIPPED] 24WMH08807.jpg\n",
      "Processing: 24WMR08820.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Processing: 24WMR08821.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Processing: 24WMR08822.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Processing: 24WMR08824.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Processing: 24WMR08826.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Processing: 24WMR08827.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Processing: 24WMR08828.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Processing: 24WMR08829.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Processing: 24WMR08831.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Processing: 24WMR08832.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Processing: 24WMR08833.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Processing: 24WMR08834.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Processing: 24WMR08835.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Processing: 24WMR08837.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Processing: 24WMR08838.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Processing: 24WMR08841.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Processing: 24WMR08843.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Processing: 24WMR08861.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Processing: 24WMR08863.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Processing: 24WMR08866.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Processing: 24WMR08870.jpeg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "âœ… Loaded 22 valid faces from Photo folder.\n",
      "ğŸ“· Press 'q' to quit...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "Captured face after being still for 2 seconds:\n",
      "  24WMR08866: 73.00% similarity\n",
      "  24WMR08863: 54.69% similarity\n",
      "  24WMR08832: 53.16% similarity\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Initialize detector and embedder\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()\n",
    "\n",
    "def l2_normalize(x):\n",
    "    return x / np.linalg.norm(x)\n",
    "\n",
    "# Folder containing known faces\n",
    "photo_dir =\"photos\"\n",
    "\n",
    "# Get embedding from image path\n",
    "def get_face_embedding(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[WARNING] Failed to load image: {img_path}\")\n",
    "        return None\n",
    "    \n",
    "    results = detector.detect_faces(img)\n",
    "    if len(results) == 0:\n",
    "        print(f\"[INFO] No face detected in: {img_path}\")\n",
    "        return None\n",
    "    \n",
    "    face = results[0]\n",
    "    x, y, w, h = face['box']\n",
    "    x, y = max(0, x), max(0, y)\n",
    "    \n",
    "    face_img = img[y:y+h, x:x+w]\n",
    "    face_img = cv2.resize(face_img, (160, 160))\n",
    "    \n",
    "    embedding = embedder.embeddings([face_img])[0]\n",
    "    embedding = l2_normalize(embedding)\n",
    "    return embedding\n",
    "\n",
    "# Build the face database from photo directory\n",
    "def build_face_database(folder):\n",
    "    database = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            path = os.path.join(folder, file)\n",
    "            name = os.path.splitext(file)[0]\n",
    "            print(f\"Processing: {file}\")\n",
    "            embedding = get_face_embedding(path)\n",
    "            if embedding is not None:\n",
    "                database[name] = embedding\n",
    "            else:\n",
    "                print(f\"[SKIPPED] {file}\")\n",
    "    return database\n",
    "\n",
    "face_database = build_face_database(photo_dir)\n",
    "print(f\"âœ… Loaded {len(face_database)} valid faces from Photo folder.\")\n",
    "\n",
    "# Get top N matches with cosine similarity\n",
    "def get_top_matches(face_img, database, top_n=3):\n",
    "    face_img = cv2.resize(face_img, (160, 160))\n",
    "    embedding = embedder.embeddings([face_img])[0]\n",
    "    embedding = l2_normalize(embedding)\n",
    "\n",
    "    similarities = []\n",
    "    for name, db_emb in database.items():\n",
    "        sim_score = cosine_similarity([embedding], [db_emb])[0][0]  # Higher is better\n",
    "        similarities.append((name, sim_score))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Start webcam recognition with stillness detection\n",
    "video = cv2.VideoCapture(0)\n",
    "print(\"ğŸ“· Press 'q' to quit...\")\n",
    "\n",
    "# Parameters for stillness detection\n",
    "still_threshold = 10  # max movement in pixels to consider still\n",
    "still_duration = 2    # seconds to hold still before capture\n",
    "\n",
    "last_face_pos = None\n",
    "still_start_time = None\n",
    "captured = False\n",
    "top_matches = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = detector.detect_faces(frame)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        # No face detected, reset everything\n",
    "        last_face_pos = None\n",
    "        still_start_time = None\n",
    "        captured = False\n",
    "        top_matches = []\n",
    "    else:\n",
    "        # Only process the largest face (closest)\n",
    "        largest_face = max(results, key=lambda f: f['box'][2] * f['box'][3])\n",
    "        x, y, w, h = largest_face['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "\n",
    "        # Calculate movement from last frame\n",
    "        if last_face_pos is not None:\n",
    "            lx, ly, lw, lh = last_face_pos\n",
    "            movement = abs(x - lx) + abs(y - ly) + abs(w - lw) + abs(h - lh)\n",
    "        else:\n",
    "            movement = None\n",
    "\n",
    "        if movement is not None and movement < still_threshold:\n",
    "            if still_start_time is None:\n",
    "                still_start_time = time.time()\n",
    "            else:\n",
    "                elapsed = time.time() - still_start_time\n",
    "                remaining = int(still_duration - elapsed) + 1\n",
    "\n",
    "                # Draw countdown timer label\n",
    "                countdown_label = f\"Hold still... {remaining}s\"\n",
    "                (label_width, label_height), baseline = cv2.getTextSize(countdown_label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "                cv2.rectangle(frame, (x, y - label_height - baseline - 10), (x + label_width, y), (0, 255, 255), cv2.FILLED)\n",
    "                cv2.putText(frame, countdown_label, (x, y - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "\n",
    "                if elapsed >= still_duration and not captured:\n",
    "                    # Capture face image with margin\n",
    "                    margin = 10\n",
    "                    x1 = max(0, x - margin)\n",
    "                    y1 = max(0, y - margin)\n",
    "                    x2 = min(frame.shape[1], x + w + margin)\n",
    "                    y2 = min(frame.shape[0], y + h + margin)\n",
    "                    face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                    # Compare with database and get top matches\n",
    "                    top_matches = get_top_matches(face_img, face_database)\n",
    "\n",
    "                    print(f\"\\nCaptured face after being still for {still_duration} seconds:\")\n",
    "                    for match_name, sim in top_matches:\n",
    "                        print(f\"  {match_name}: {sim * 100:.2f}% similarity\")\n",
    "\n",
    "                    captured = True\n",
    "        else:\n",
    "            # Movement too big or first detection: reset timer and capture flag\n",
    "            still_start_time = None\n",
    "            captured = False\n",
    "            top_matches = []\n",
    "\n",
    "        last_face_pos = (x, y, w, h)\n",
    "\n",
    "        # Draw bounding box and label\n",
    "        if captured and top_matches:\n",
    "            # Green box with best match label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            best_name, best_sim = top_matches[0]\n",
    "            label = f\"{best_name} ({best_sim*100:.1f}%)\"\n",
    "            (label_width, label_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            cv2.rectangle(frame, (x, y - label_height - baseline - 5), (x + label_width, y), (0, 255, 0), cv2.FILLED)\n",
    "            cv2.putText(frame, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "        else:\n",
    "            # Yellow box while waiting\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Face Recognition with MTCNN + FaceNet', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29b1c4",
   "metadata": {},
   "source": [
    "# Barcode Detection - Gan Khai Li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b96c1",
   "metadata": {},
   "source": [
    "# Barcode Detection - Kit Chin Jie Ying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87a1ef",
   "metadata": {},
   "source": [
    "# Real Time Human Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c024f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics, deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefed7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Configuration\n",
    "model_path = \"yolov8n.pt\"\n",
    "threshold = 0.4\n",
    "max_frames = 300\n",
    "\n",
    "# Load YOLO model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = YOLO(model_path)\n",
    "model.to(device)\n",
    "\n",
    "# Deep SORT tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,\n",
    "    n_init=3,\n",
    "    max_iou_distance=0.7,\n",
    "    nms_max_overlap=1.0,\n",
    "    max_cosine_distance=0.2,\n",
    "    nn_budget=None,\n",
    "    embedder=\"mobilenet\",\n",
    "    half=torch.cuda.is_available(),\n",
    "    bgr=True,\n",
    ")\n",
    "\n",
    "def draw_box_with_label(img, tlbr, label, color=(0, 255, 0)):\n",
    "    x1, y1, x2, y2 = map(int, tlbr)\n",
    "    cv.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    (tw, th), _ = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "    cv.rectangle(img, (x1, y1 - th - 6), (x1 + tw + 6, y1), color, -1)\n",
    "    cv.putText(img, label, (x1 + 3, y1 - 6),\n",
    "                cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "# Initialize variable\n",
    "video = cv.VideoCapture(0)\n",
    "fps_smooth = None\n",
    "person_class_id = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if not ret: \n",
    "        break\n",
    "\n",
    "    frame = cv.flip(frame, 1)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Detect person\n",
    "    results = model.predict(\n",
    "        frame,\n",
    "        conf=threshold,\n",
    "        iou=0.45,\n",
    "        verbose=False,\n",
    "        classes=[person_class_id],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Convert the YOLO detections\n",
    "    detections = []\n",
    "    if len(results):\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            for box in r.boxes:\n",
    "                xyxy = box.xyxy[0].cpu().numpy()\n",
    "                conf = float(box.conf[0].cpu().numpy())\n",
    "                detections.append([xyxy.tolist(), conf, \"person\"])\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for trk in tracks:\n",
    "        if not trk.is_confirmed() or trk.time_since_update > 0:\n",
    "            continue\n",
    "        track_id = trk.track_id\n",
    "        tlbr = trk.to_tlbr()\n",
    "        label = f\"ID {track_id}\"\n",
    "        draw_box_with_label(frame, tlbr, label, color=(0, 255, 0))\n",
    "\n",
    "    # FPS\n",
    "    dt = time.time() - t0\n",
    "    fps = 1.0 / dt if dt > 0 else 0.0\n",
    "    fps_smooth = fps if fps_smooth is None else fps_smooth * 0.9 + fps * 0.1\n",
    "    cv.putText(frame, f\"FPS: {fps_smooth:.1f}\", (10, 30),\n",
    "                cv.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "    \n",
    "    cv.imshow(\"Real Time Human Tracking\", frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950e1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
